{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolo_object_counting_opencv_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd_UaH09FB-A",
        "outputId": "dd4d1118-4cf3-4334-e455-46458df3486b"
      },
      "source": [
        "# get yolo v3 weights and coco dataset object names\n",
        "\n",
        "!mkdir model\n",
        "%cd model/\n",
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names wget https://pjreddie.com/media/files/yolov3.weights\n",
        "%cd .."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/model\n",
            "--2021-01-20 16:30:54--  https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8342 (8.1K) [text/plain]\n",
            "Saving to: ‘yolov3.cfg’\n",
            "\n",
            "yolov3.cfg          100%[===================>]   8.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-01-20 16:30:55 (64.9 MB/s) - ‘yolov3.cfg’ saved [8342/8342]\n",
            "\n",
            "--2021-01-20 16:30:55--  https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\n",
            "Reusing existing connection to raw.githubusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 625 [text/plain]\n",
            "Saving to: ‘coco.names’\n",
            "\n",
            "coco.names          100%[===================>]     625  --.-KB/s    in 0s      \n",
            "\n",
            "2021-01-20 16:30:55 (40.7 MB/s) - ‘coco.names’ saved [625/625]\n",
            "\n",
            "--2021-01-20 16:30:55--  http://wget/\n",
            "Resolving wget (wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘wget’\n",
            "--2021-01-20 16:30:55--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M   684KB/s    in 6m 10s  \n",
            "\n",
            "2021-01-20 16:37:05 (654 KB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
            "\n",
            "FINISHED --2021-01-20 16:37:05--\n",
            "Total wall clock time: 6m 11s\n",
            "Downloaded: 3 files, 237M in 6m 10s (654 KB/s)\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8HVgxSuuVat"
      },
      "source": [
        "# import required libraries\n",
        "\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fItMqX4vhPLf",
        "outputId": "3c07c81a-1cbf-474c-932e-648d9b8871ae"
      },
      "source": [
        "# mount drive if neccessary\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgAHxCwnueBV"
      },
      "source": [
        "# function to get boxes confidences and returning\n",
        "\n",
        "def extract_boxes_confidences_classids(outputs, confidence, width, height):\n",
        "    boxes = []\n",
        "    confidences = []\n",
        "    classIDs = []\n",
        "    object_counts = np.zeros(85)\n",
        "\n",
        "    for output in outputs:\n",
        "        for detection in output:            \n",
        "            scores = detection[5:]\n",
        "            classID = np.argmax(scores)\n",
        "            conf = scores[classID]\n",
        "            \n",
        "            if conf > confidence:\n",
        "                box = detection[0:4] * np.array([width, height, width, height])\n",
        "                centerX, centerY, w, h = box.astype('int')\n",
        "\n",
        "                x = int(centerX - (w / 2))\n",
        "                y = int(centerY - (h / 2))\n",
        "\n",
        "                boxes.append([x, y, int(w), int(h)])\n",
        "                confidences.append(float(conf))\n",
        "                classIDs.append(classID)\n",
        "\n",
        "    return boxes, confidences, classIDs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyZsk6AB5g2O"
      },
      "source": [
        "# draw bounding boxes and object names and counts\n",
        "\n",
        "def draw_bounding_boxes(image, boxes, confidences, classIDs, idxs, colors):\n",
        "    object_counts = np.zeros(85)\n",
        "\n",
        "    if len(idxs) > 0:\n",
        "        for i in idxs.flatten():\n",
        "            x, y = boxes[i][0], boxes[i][1]\n",
        "            w, h = boxes[i][2], boxes[i][3]\n",
        "\n",
        "            color = [int(c) for c in colors[classIDs[i]]]\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
        "            #print(labels[classIDs[i]])\n",
        "            object_counts[classIDs[i]] += 1\n",
        "            text = \"{}: {:.4f}\".format(labels[classIDs[i]], confidences[i])\n",
        "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "            #print(object_counts)\n",
        "        offset = 0\n",
        "\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        for i in range(len(labels)):\n",
        "            if object_counts[i] != 0:\n",
        "                print(labels[i])\n",
        "                print(object_counts[i])\n",
        "                offset += 40\n",
        "                text = labels[i] + '->' + str(int(object_counts[i]))\n",
        "                cv2.putText(image, text, (20, offset), font, 1, (255, 0, 0), 2)\n",
        "                \n",
        "\n",
        "    return image, object_counts"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvYuPqKputoy"
      },
      "source": [
        "# function to use opencv dnn module to get predictions from yolo v3 weights\n",
        "\n",
        "def make_prediction(net, layer_names, labels, image, confidence, threshold):\n",
        "    height, width = image.shape[:2]\n",
        "    \n",
        "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    outputs = net.forward(layer_names)\n",
        "\n",
        "    boxes, confidences, classIDs = extract_boxes_confidences_classids(outputs, confidence, width, height)\n",
        "\n",
        "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, confidence, threshold)\n",
        "\n",
        "    return boxes, confidences, classIDs, idxs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9us5zctusL4"
      },
      "source": [
        "# not used for future purposes\n",
        "\n",
        "'''def count_objects(classIDs, idxs, counts):\n",
        "    if len(idxs) > 0:\n",
        "        for i in idxs.flatten():\n",
        "            counts[classIDs[i]] += 1\n",
        "    \n",
        "        for i in range(len(labels)):\n",
        "            if counts[classIDs[i]] > 0:\n",
        "                print(labels[classIDs[i]])\n",
        "                print(counts[classIDs[i]])\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o7beQOiFwi9",
        "outputId": "6b86650f-5d18-4173-9614-f9e8bba9d901"
      },
      "source": [
        "# Main code and parameters to run object counting with defined parameters ob images and videos\n",
        "\n",
        "weights = '/content/model/yolov3.weights'\n",
        "config = '/content/model/yolov3.cfg'\n",
        "labels = '/content/model/coco.names'\n",
        "\n",
        "confidence = 0.30\n",
        "threshold = 0.30\n",
        "\n",
        "use_gpu = False\n",
        "save = True\n",
        "image_path = ''\n",
        "video_path = ''\n",
        "show = False\n",
        "\n",
        "labels = open(labels).read().strip().split('\\n')\n",
        "\n",
        "colors = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
        "\n",
        "net = cv2.dnn.readNetFromDarknet(config, weights)\n",
        "\n",
        "if use_gpu:\n",
        "    print('Using GPU')\n",
        "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
        "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
        "\n",
        "layer_names = net.getLayerNames()\n",
        "layer_names = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "\n",
        "if image_path != '':\n",
        "    image = cv2.imread(image_path)\n",
        "    print(image.shape)\n",
        "    boxes, confidences, classIDs, idxs = make_prediction(net, layer_names, labels, image, confidence, threshold)\n",
        "\n",
        "    image, counts = draw_bounding_boxes(image, boxes, confidences, classIDs, idxs, colors)\n",
        "\n",
        "    if show:\n",
        "        cv2.imshow('YOLO Object Detection', image)\n",
        "        cv2.waitKey(0)\n",
        "    \n",
        "    if save:\n",
        "        cv2.imwrite('./image_output.png', image)\n",
        "\n",
        "else:\n",
        "    if video_path != '':\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if (cap.isOpened()== False):\n",
        "            print(\"Error opening video stream or file\")\n",
        "    else:\n",
        "        cap = cv2.VideoCapture(0)\n",
        "\n",
        "    if save:\n",
        "        width = int(cap.get(3))\n",
        "        height = int(cap.get(4))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        name = 'video_output.mp4'\n",
        "        out = cv2.VideoWriter(f'./{name}', cv2.VideoWriter_fourcc('M','J','P','G'), fps, (width, height))\n",
        "  \n",
        "    while cap.isOpened():\n",
        "        \n",
        "            ret, image = cap.read()\n",
        "\n",
        "            if not ret:\n",
        "                print('Video file finished.')\n",
        "                break\n",
        "\n",
        "            boxes, confidences, classIDs, idxs = make_prediction(net, layer_names, labels, image, confidence, threshold)\n",
        "\n",
        "            image, counts = draw_bounding_boxes(image, boxes, confidences, classIDs, idxs, colors)\n",
        "\n",
        "            if show:\n",
        "                cv2.imshow('YOLO Object Detection', image)\n",
        "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                    break\n",
        "            \n",
        "            if save:\n",
        "                out.write(image)\n",
        "\n",
        "    cap.release()\n",
        "    if save:\n",
        "        out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "cell phone\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "cell phone\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "cell phone\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "3.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "person\n",
            "2.0\n",
            "car\n",
            "1.0\n",
            "Video file finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xXhUeVFuO3V"
      },
      "source": [
        "#!cp \"/content/drive/Shareddrives/hasib/cv2.cpython-36m-x86_64-linux-gnu.so\" ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yf-OUdBQAdo"
      },
      "source": [
        "#!cp \"/content/drive/Shareddrives/hasib/cv2_gpu\" ."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}